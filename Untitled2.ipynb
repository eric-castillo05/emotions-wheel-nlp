{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84cafa6-57a6-43c0-863e-0d3ca22bb4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91d47dc-bb9f-481a-aa9a-38f57657fc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e60243-34a6-4870-a498-f90200682a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pl.read_parquet('hf://datasets/Jsevisal/go_emotions_wheel/' + splits['train'])\n",
    "df_test = pl.read_parquet('hf://datasets/Jsevisal/go_emotions_wheel/' + splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629953aa-cf5d-4316-b4eb-f69e21850e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8bdda8b-0ce2-454e-8933-60e3d1d7d352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_train = list(nlp.pipe(df_train.select('text').to_series().to_list()))\n",
    "docs_test = list(nlp.pipe(df_test.select('text').to_series().to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9a94df-7782-4f51-a687-d0f4a5bab2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    "    return [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05c9145-0b26-4457-a759-7324db972f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_texts_train = [' '.join(preprocess_text(doc)) for doc in docs_train]\n",
    "processed_texts_test = [' '.join(preprocess_text(doc)) for doc in docs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bab5aa-cd20-43b6-bd50-f424d91e5461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0c4b12c-3441-4ed1-8287-24463a4ee537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_raw = df_train['labels'].to_list()\n",
    "y_test_raw = df_test['labels'].to_list()\n",
    "\n",
    "all_labels_combined = y_train_raw + y_test_raw\n",
    "all_unique_label_ids = sorted(list(set(item for sublist in all_labels_combined for item in sublist)))\n",
    "mlb = MultiLabelBinarizer(classes=all_unique_label_ids)\n",
    "mlb.fit(all_labels_combined)\n",
    "\n",
    "y_train_multilabel = mlb.transform(y_train_raw)\n",
    "y_test_multilabel = mlb.transform(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89dedf5a-0951-48ff-8ce4-b7e896dbd7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6814000-2e32-4bdf-a89b-8f883bfd8af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b12fbdcb-43f6-4f2e-bf6f-6a54987de358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = tfidf.fit_transform(processed_texts_train)\n",
    "x_test = tfidf.transform(processed_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "079e209c-0480-423c-98d4-d71ae5898c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiLabelNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.5):\n",
    "        super(MultiLabelNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        self.dropout_rate = nn.Dropout(dropout_rate)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_rate(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout_rate(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc34690e-d6d9-4467-bf2d-b5ef858cc3d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelNN(\n",
       "  (fc1): Linear(in_features=5000, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (dropout_rate): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiLabelNN(input_dim=5000, output_dim=y_train_multilabel.shape[1])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edf4958c-70f9-406b-bd9b-9822bd8feb06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43410"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0b7282f-4ce1-4639-a82e-e2d320792f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = np.sum(y_train_multilabel, axis=0) \n",
    "total = y_train_multilabel.shape[0]\n",
    "\n",
    "counts[counts == 0] = 1\n",
    "pos_weight = torch.tensor((total - counts) / counts, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44d77e7a-44c4-4aee-88de-ea3d6cd90007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, pos_weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # BCE with logits\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none', pos_weight=self.pos_weight\n",
    "        )\n",
    "        # Probabilities\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "\n",
    "        loss = focal_weight * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2be88fa5-27b0-4f21-8b0f-3bb6d577019b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "focal_criterion = FocalLoss(gamma=2, pos_weight=pos_weight.to(device)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ad9cef2-7e3e-4f52-a0b4-3f2b16b8dcdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_tensor = torch.tensor(x_train.toarray(), dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(y_train_multilabel, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "845defeb-67e1-4bea-a7fd-d3a17de0b963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.2369\n",
      "Epoch 2 | Loss: 0.1910\n",
      "Epoch 3 | Loss: 0.2003\n",
      "Epoch 4 | Loss: 0.1248\n",
      "Epoch 5 | Loss: 0.1162\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5): \n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = focal_criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc4c7267-6f4c-4a3a-8d0b-af74b825670b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78      1650\n",
      "           1       0.20      0.58      0.29       474\n",
      "           2       0.27      0.76      0.39        98\n",
      "           3       0.24      0.60      0.35       677\n",
      "           4       0.27      0.67      0.39       379\n",
      "           5       0.53      0.79      0.63      1787\n",
      "           6       0.16      0.54      0.25        83\n",
      "           7       0.28      0.66      0.39       726\n",
      "           8       0.13      0.65      0.22       123\n",
      "\n",
      "   micro avg       0.38      0.73      0.50      5997\n",
      "   macro avg       0.32      0.67      0.41      5997\n",
      "weighted avg       0.47      0.73      0.55      5997\n",
      " samples avg       0.46      0.74      0.54      5997\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_tensor = torch.tensor(x_test.toarray(), dtype=torch.float32).to(device)\n",
    "    logits = model(test_tensor)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_multilabel, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e6458-3d5e-4343-9d0d-5c73be12bf3d",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40aa8322-843e-40d3-bce6-84d6ae7d5799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "\u001b[K     |████████████████████████████████| 395 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic>=1.5.0\n",
      "  Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 85.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (2.0.2)\n",
      "Collecting sqlalchemy>=1.4.2\n",
      "  Downloading sqlalchemy-2.0.41-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 61.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (24.0)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tomli in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (2.0.1)\n",
      "Collecting Mako\n",
      "  Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 9.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (580 kB)\n",
      "\u001b[K     |████████████████████████████████| 580 kB 72.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Installing collected packages: greenlet, sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.2 colorlog-6.9.0 greenlet-3.2.3 optuna-4.4.0 sqlalchemy-2.0.41\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "08304e62-21b4-4e5c-b5b6-3c232d2c5213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f82e6682-83b2-495c-bea8-32b12c7475c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "X_tensor_test = torch.tensor(x_test.toarray(), dtype=torch.float32)\n",
    "Y_tensor_test = torch.tensor(y_test_multilabel, dtype=torch.float32)\n",
    "val_dataset = TensorDataset(X_tensor_test, Y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f6e44ac-2a56-421f-86b2-1b15c1c49f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-1)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64])\n",
    "    early_stop_patience = trial.suggest_int('early_stop_patience', 2, 6)\n",
    "    max_length = trial.suggest_categorical('max_length', [32, 64, 128, 256])\n",
    "    gradient_clip = trial.suggest_uniform('gradient_clip', 0.5, 5.0)\n",
    "    eps = trial.suggest_loguniform('eps', 1e-10, 1e-6)\n",
    "    correct_bias = trial.suggest_categorical('correct_bias', [True, False])\n",
    "    \n",
    "    focal_gamma = trial.suggest_float('focal_gamma', 0.5, 5.0) \n",
    "    pos_weight = trial.suggest_categorical('pos_weight', [None, 1.0, 2.0, 5.0])\n",
    "        \n",
    "    model = MultiLabelNN(input_dim=5000, output_dim=y_train_multilabel.shape[1], dropout_rate=dropout_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps=eps)\n",
    "    \n",
    "    criterion = FocalLoss(gamma=focal_gamma, pos_weight=pos_weight)\n",
    "    \n",
    "    X_tensor = torch.tensor(x_train.toarray(), dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(y_train_multilabel, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = focal_criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    all_val_predictions = []\n",
    "    all_val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader:\n",
    "            outputs = model(data)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predicted = (probabilities > 0.5).int()\n",
    "            all_val_predictions.extend(predicted.cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays for sklearn metrics\n",
    "    y_true = np.array(all_val_labels)\n",
    "    y_pred = np.array(all_val_predictions)\n",
    "\n",
    "    # Choose an appropriate metric to maximize\n",
    "    # For multi-label with imbalance, macro F1 is often preferred.\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    return macro_f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee0e03df-5a2b-4ed3-b6d8-88962099760c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 00:13:48,239] A new study created in memory with name: no-name-33fbc557-f52c-4c9f-8241-09c3a4a648bc\n",
      "/tmp/ipykernel_109/2723954742.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)\n",
      "/tmp/ipykernel_109/2723954742.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-1)\n",
      "/tmp/ipykernel_109/2723954742.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "/tmp/ipykernel_109/2723954742.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gradient_clip = trial.suggest_uniform('gradient_clip', 0.5, 5.0)\n",
      "/tmp/ipykernel_109/2723954742.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eps = trial.suggest_loguniform('eps', 1e-10, 1e-6)\n",
      "[I 2025-07-08 00:14:52,572] Trial 0 finished with value: 0.0 and parameters: {'learning_rate': 0.00012239584462084627, 'weight_decay': 0.005034300558684191, 'dropout_rate': 0.21217268470157413, 'batch_size': 64, 'early_stop_patience': 6, 'max_length': 32, 'gradient_clip': 3.8686292419697765, 'eps': 9.744377779783682e-09, 'correct_bias': True, 'focal_gamma': 2.0421382548477567, 'pos_weight': 2.0}. Best is trial 0 with value: 0.0.\n",
      "/tmp/ipykernel_109/2723954742.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)\n",
      "/tmp/ipykernel_109/2723954742.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-1)\n",
      "/tmp/ipykernel_109/2723954742.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "/tmp/ipykernel_109/2723954742.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gradient_clip = trial.suggest_uniform('gradient_clip', 0.5, 5.0)\n",
      "/tmp/ipykernel_109/2723954742.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eps = trial.suggest_loguniform('eps', 1e-10, 1e-6)\n",
      "[I 2025-07-08 00:22:54,094] Trial 1 finished with value: 0.0 and parameters: {'learning_rate': 0.00012063451991274709, 'weight_decay': 0.043458792387025685, 'dropout_rate': 0.36614669204381145, 'batch_size': 8, 'early_stop_patience': 3, 'max_length': 256, 'gradient_clip': 3.52354449966105, 'eps': 3.107502732116823e-08, 'correct_bias': True, 'focal_gamma': 4.791252786497491, 'pos_weight': 5.0}. Best is trial 0 with value: 0.0.\n",
      "/tmp/ipykernel_109/2723954742.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)\n",
      "/tmp/ipykernel_109/2723954742.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-1)\n",
      "/tmp/ipykernel_109/2723954742.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "/tmp/ipykernel_109/2723954742.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gradient_clip = trial.suggest_uniform('gradient_clip', 0.5, 5.0)\n",
      "/tmp/ipykernel_109/2723954742.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  eps = trial.suggest_loguniform('eps', 1e-10, 1e-6)\n",
      "[I 2025-07-08 00:24:00,076] Trial 2 finished with value: 0.0 and parameters: {'learning_rate': 1.884010354023624e-05, 'weight_decay': 0.0019097540113125787, 'dropout_rate': 0.4296091474943039, 'batch_size': 64, 'early_stop_patience': 2, 'max_length': 128, 'gradient_clip': 1.0585973793155738, 'eps': 1.3480965871552945e-10, 'correct_bias': True, 'focal_gamma': 1.4230805131185322, 'pos_weight': 1.0}. Best is trial 0 with value: 0.0.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2cb2898-4fa0-4405-be78-309f9f5496ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'learning_rate': 0.00012239584462084627, 'weight_decay': 0.005034300558684191, 'dropout_rate': 0.21217268470157413, 'batch_size': 64, 'early_stop_patience': 6, 'max_length': 32, 'gradient_clip': 3.8686292419697765, 'eps': 9.744377779783682e-09, 'correct_bias': True, 'focal_gamma': 2.0421382548477567, 'pos_weight': 2.0}\n",
      "Mejor valor de la métrica: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejores hiperparámetros:\", study.best_params)\n",
    "print(\"Mejor valor de la métrica:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b37ac406-6f31-4dfc-9490-dbb8d6c2f04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params ={'learning_rate': 0.00012239584462084627, 'weight_decay': 0.005034300558684191, 'dropout_rate': 0.21217268470157413, 'batch_size': 64, 'early_stop_patience': 6, 'max_length': 32, 'gradient_clip': 3.8686292419697765, 'eps': 9.744377779783682e-09, 'correct_bias': True, 'focal_gamma': 2.0421382548477567, 'pos_weight': 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f39c7b7b-e8f9-4e50-b65f-7315666c87c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MultiLabelNN(input_dim=5000, output_dim=y_train_multilabel.shape[1], dropout_rate=best_params['dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bac3cf83-6527-461c-8848-2832cb17f89d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=best_params['learning_rate'],\n",
    "    weight_decay=best_params['weight_decay'],\n",
    "    eps=best_params['eps']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1166519-86e6-4d7e-95f1-e25a8a9175b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = FocalLoss(gamma=best_params['focal_gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba3fc0fa-6888-4d82-8d01-7bf13bd7631b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ed254bd-79cc-4771-8d8e-be258c251021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "959c0e98-9456-4269-a163-feed284352c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(int(best_params['eps'])+1):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), \n",
    "            best_params['gradient_clip']\n",
    "        )\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "108d9698-f17e-46bf-aced-c75d1ede5a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelNN(\n",
       "  (fc1): Linear(in_features=5000, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (dropout_rate): Dropout(p=0.21217268470157413, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9bb8e5d1-17f7-46c7-b1af-2c672c57933b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.07      0.13      1650\n",
      "           1       0.00      0.00      0.00       474\n",
      "           2       0.00      0.00      0.00        98\n",
      "           3       0.00      0.00      0.00       677\n",
      "           4       0.00      0.00      0.00       379\n",
      "           5       0.00      0.00      0.00      1787\n",
      "           6       0.00      0.00      0.00        83\n",
      "           7       0.00      0.00      0.00       726\n",
      "           8       0.00      0.00      0.00       123\n",
      "\n",
      "   micro avg       0.98      0.02      0.04      5997\n",
      "   macro avg       0.11      0.01      0.01      5997\n",
      "weighted avg       0.27      0.02      0.04      5997\n",
      " samples avg       0.02      0.02      0.02      5997\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for data, labels in val_loader:\n",
    "        outputs = model(data)\n",
    "        \n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predicted = (probabilities > 0.5).int()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "print(classification_report(np.array(all_labels), np.array(all_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f61420-d87a-4156-ba94-f556c88c89c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
